{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be30058-aa56-45e9-8ce3-9e79b22957ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 12\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "from utils import generator, online_matchmaking, metrics, optimizers, actions, obj_functions, dyn_systems\n",
    "from utils.base import Resource\n",
    "import const_define as cd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff609b4-3f58-4793-9272-2979409c24d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86717c65-a444-4c45-9c8d-fee4a0d54470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed=424242\n",
    "dataset = 'strong_unbalanced'\n",
    "\n",
    "labelsAI = {i:f'AILabel{i+1}'for i in range(9)}\n",
    "\n",
    "countries = {i+9:f'Country{i+1}'for i in range(5)}\n",
    "\n",
    "r_rank, q_rank = None, None\n",
    "\n",
    "# Balanced\n",
    "if dataset == 'balanced':\n",
    "    q_countries = (1/len(countries),) * len(countries)\n",
    "    r_countries = q_countries\n",
    "# Mild Unbalanced\n",
    "elif dataset == 'mild_unbalanced':\n",
    "    r_countries = (0.15, 0.15, 0.4, 0.15, 0.15)\n",
    "    q_countries = (1/len(countries),) * len(countries)\n",
    "# Strong Unbalanced\n",
    "elif dataset == 'strong_unbalanced':\n",
    "    r_countries = (0.05, 0.05, 0.8, 0.05, 0.05)\n",
    "    q_countries = (1/len(countries),) * len(countries)\n",
    "else:\n",
    "    raise ValueError()\n",
    "\n",
    "g = generator.StrwaiDataGenerator(labelsAI,\n",
    "                        countries,\n",
    "                        q_rank=q_rank,\n",
    "                        r_rank=r_rank,\n",
    "                        q_countries=q_countries,\n",
    "                        r_countries=r_countries,\n",
    "                        seed=seed)\n",
    "\n",
    "#print(g)\n",
    "n_resources = 40\n",
    "n_queries = 100\n",
    "queries_scores, resources_scores = g.generate_data(n_queries, n_resources)\n",
    "g.plot_data(queries_scores, resources_scores, name=dataset, save=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1824a495-974e-411d-bb44-56eee5a8dc05",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test with Different Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde2e46e-ffc1-4159-8278-3c3bbf4b80d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Resources\n",
    "resources_ids = list(range(len(resources_scores)))\n",
    "resources_names = [f'res_{i}' for i in resources_ids]\n",
    "resources_descr = [f'descr_{i}' for i in resources_ids]\n",
    "resources_scores_dict = [{str(l):resources_scores[i,l] for l in range(g.n_labelsAI + g.n_countries)} for i in resources_ids]\n",
    "\n",
    "resources_list = [Resource(name=resources_names[i], identifier=resources_ids[i], description=resources_descr[i],\n",
    "                 scores=resources_scores_dict[i]) for i in resources_ids]\n",
    "# Create Quieries\n",
    "queries_scores_list = [{str(l):queries_scores[i,l] for l in range(g.n_labelsAI + g.n_countries)} for i in range(len(queries_scores))]\n",
    "# Define matching algorithm\n",
    "matching_metrics = online_matchmaking.CosineSimilarityScore()\n",
    "mm_alg = online_matchmaking.OnlineMatchmaking(matching_metrics)\n",
    "mm_alg.loadResources(resources_list)\n",
    "\n",
    "# Listing protected label idxs\n",
    "protected_labels = [9,10,11,12,13]\n",
    "# Defining resources scores matrix\n",
    "resources_scores = np.zeros((len(resources_list),g.n_labelsAI + g.n_countries))\n",
    "for i in range(len(resources_list)):\n",
    "    resources_scores[i] = np.array([resources_list[i].scores[str(j)] for j in range(g.n_labelsAI + g.n_countries)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49bded7-0708-4d93-9c93-6dace5d37e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = {}\n",
    "path = os.path.join(cd.PROJECT_DIR,'records', f'{dataset}/')\n",
    "imgpath = os.path.join(cd.PROJECT_DIR,'images', f'{dataset}/')\n",
    "records_list = ['true_rank',  # true rank (unmodified output of the matchmaking alg)\n",
    "                'actioned_rank', # rank resulting from the application of the the actions found in the previous search to the true rank\n",
    "                'provided_metrics',  # metrics of the actioned rank (which is the one provided to the user)\n",
    "                'approx_metrics', # metrics of the approximated rank (it's obtained by applying the actions found in the current search to the actioned rank)\n",
    "                'state', # record of the dynamical system evolution for each query (it's equal to the metrics itself if no search was performed)\n",
    "                'cost_fn']  # approximation error (it's equal to -1 if no search was performed)\n",
    "\n",
    "for file in records_list:\n",
    "    fname = os.path.join(path,file + '.pkl')\n",
    "    # Load existing records (if any)\n",
    "    if os.path.isfile(fname):\n",
    "        records[file] = pd.read_pickle(fname)\n",
    "        print(f'Records loaded for {file}, with {len(records[file].columns)} experiments')\n",
    "    else:\n",
    "        print(f'No records found for {file} in {path}')\n",
    "        records[file] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ce892-6aa3-4371-95af-09236e145023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define desired dynamical behaviour\n",
    "A = np.array([[0.5, 0.],\n",
    "              [0., 0.5]])\n",
    "# Fairness thresholds\n",
    "# DIDI and rank distance\n",
    "mu_dict = {0:np.array([1.,1.]),\n",
    "           1:np.array([0., 1.]),\n",
    "           2:np.array([1., 0.]),\n",
    "          3:np.array([0.5, 0.2]),\n",
    "          4:np.array([0.4,0.2]), \n",
    "          5:np.array([0.3,0.2]),\n",
    "          6:np.array([0.1,0.1])} \n",
    "\n",
    "previous_queries_method = ['actionable'] \n",
    "\n",
    "for mu in mu_dict:\n",
    "    \n",
    "    if mu in records['true_rank'].keys():\n",
    "        print('Already existing experiment')\n",
    "        continue\n",
    "    else:\n",
    "        print(f'Test framework with threshold conf. No. {mu}')\n",
    "\n",
    "        dyn_sys=dyn_systems.DynSystemMin(A,mu)\n",
    "        for file in records_list:\n",
    "            records[file][mu] = {k:None for k in previous_queries_method}\n",
    "            \n",
    "        for mode in previous_queries_method:\n",
    "\n",
    "            cd.set_seed(seed)\n",
    "            # Creating DIDI metrics object\n",
    "            didi_obj = metrics.DIDI(seed,resources_scores, protected_labels, impact_function=metrics.power_law)\n",
    "\n",
    "            # Define the metrics\n",
    "            metrics_dict = OrderedDict(DIDI=didi_obj,\n",
    "                           rank_dist=metrics.rank_similarity)\n",
    "            metrics_obj = metrics.Metrics(metrics_dict, seed=seed)\n",
    "            args_metrics = {k:{} for k in metrics_obj.metrics_dict}\n",
    "\n",
    "            # Define objective function\n",
    "            obj_fn = obj_functions.obj_fn_l2_wHistory\n",
    "            # Define args for obj function\n",
    "            metrics_weight = {'DIDI':1.,\n",
    "                           'rank_dist':1.}\n",
    "\n",
    "            obj_fn_args = {'resources_list':resources_list, \n",
    "                           'metrics':metrics_obj,\n",
    "                          'args_metrics':args_metrics,\n",
    "                          'metrics_weight':metrics_weight}\n",
    "\n",
    "\n",
    "            # Define searching algorithm\n",
    "            max_iter = 200\n",
    "            patience = 20\n",
    "            tolerance = 1e-4\n",
    "            rnd_search = optimizers.RandomWalk_wHistory(max_iter=max_iter,\n",
    "                                                n_samples=10,\n",
    "                                                decay_rate=0.04,\n",
    "                                                cost_fn=obj_fn,\n",
    "                                                 patience=patience,\n",
    "                                                 tolerance=tolerance,\n",
    "                                                mode=mode,\n",
    "                                                seed=seed,\n",
    "                                              verbose=False)\n",
    "            \n",
    "            for file in records_list:\n",
    "                if file in ['provided_metrics', 'approx_metrics']:\n",
    "                    records[file][mu][mode] = {name:[] for name in metrics_obj.metrics_dict}\n",
    "                else:\n",
    "                    records[file][mu][mode] = []\n",
    "\n",
    "            # Number of previous ranks to consider\n",
    "            K = 5 \n",
    "            counter = 0\n",
    "            # We assume to not modify the very first query rank\n",
    "            previous_flagged_resources = np.zeros((len(resources_list),),)\n",
    "\n",
    "            # Loop over the queries\n",
    "            for q in tqdm(queries_scores_list):\n",
    "                # 1) Compute original rank r of Resources resulting from q\n",
    "                true_rank = OrderedDict(mm_alg.matchOne(query_scores=q))\n",
    "                records['true_rank'][mu][mode].append(true_rank)\n",
    "                # 2) Perform actions\n",
    "                # Change flag of resources\n",
    "                resources_list_flagged = [Resource(name=resources_list[i].name, identifier=resources_list[i].identifier,\n",
    "                                                        description=resources_list[i].description,\n",
    "                                                        scores=resources_list[i].scores, at_bottom_flag=previous_flagged_resources[i]) for i in\n",
    "                                          range(len(previous_flagged_resources))]\n",
    "                # Put at the bottom\n",
    "                actioned_rank, _ = actions.at_bottom(true_rank, resources_list_flagged)\n",
    "                records['actioned_rank'][mu][mode].append(actioned_rank)\n",
    "                # 3) Compute metrics with rank and actioned rank\n",
    "                m = np.zeros((metrics_obj.n_metrics,))\n",
    "                for i,name in enumerate(metrics_obj.metrics_dict):\n",
    "                    m[i] = metrics_obj.metrics_dict[name](actioned_rank, true_rank, **args_metrics[name])\n",
    "                    records['provided_metrics'][mu][mode][name].append(m[i])\n",
    "\n",
    "                # 4) If metrics > \\mu, select resources to put at the bottom\n",
    "                if not np.all(m <=mu_dict[mu]):\n",
    "                    # 5) Random search over \\theta to minimize \n",
    "                    #the distance between the metrics with the new rank and the expected behaviour A\n",
    "\n",
    "                    # Compute dynamical state to approximate\n",
    "                    x = dyn_sys(m)\n",
    "                    records['state'][mu][mode].append(x)\n",
    "                    # Define starting point for search\n",
    "                    w0 = np.random.choice(a=[0, 1], size=(len(resources_list),), p=[0.5, 0.5])\n",
    "\n",
    "\n",
    "                    # Number of queries to be considered\n",
    "                    if counter < K:\n",
    "                        n_ranks=counter\n",
    "                    else:\n",
    "                        n_ranks=K\n",
    "\n",
    "                    # Compute state for previous queries results \n",
    "                    previous_metrics = [np.array(records['provided_metrics'][mu][mode][name][:n_ranks])[:,None] for name in metrics_obj.metrics_dict]\n",
    "                    previous_metrics = np.concatenate(previous_metrics, axis=1)\n",
    "                    x_previous = np.array([dyn_sys(prev_m) for prev_m in previous_metrics])\n",
    "                    # Define args for obj function\n",
    "                    obj_fn_args['x']=x,\n",
    "                    obj_fn_args['rank']=actioned_rank\n",
    "                    obj_fn_args['x_previous'] = x_previous\n",
    "                    # Searching\n",
    "                    flag_history, cost_history, flipping_probs, k = rnd_search(flag=w0, \n",
    "                                                                                 previous_ranks=records['actioned_rank'][mu][mode][:n_ranks], \n",
    "                                                                                 args=obj_fn_args)\n",
    "\n",
    "                    # Update set of actions\n",
    "                    previous_flagged_resources = flag_history[-1]\n",
    "                    # Store results\n",
    "                    cost = cost_history[-1][0]\n",
    "                    x_approx = cost_history[-1][1]\n",
    "                    for i, name in enumerate(metrics_obj.metrics_dict):\n",
    "                        records['approx_metrics'][mu][mode][name].append(x_approx[i]) \n",
    "                    records['cost_fn'][mu][mode].append(cost)\n",
    "                else:\n",
    "                    # There has been no approximation process, thus no update is needed for the actions\n",
    "                    # and the approx metrics is exaclty the real one\n",
    "                    for i, name in enumerate(metrics_obj.metrics_dict):\n",
    "                        records['approx_metrics'][mu][mode][name].append(m[i]) \n",
    "                    records['state'][mu][mode].append(m)\n",
    "                    records['cost_fn'][mu][mode].append(-1)\n",
    "                counter+=1\n",
    "# To dict\n",
    "for file in records_list:\n",
    "    records[file] = pd.DataFrame(records[file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987c5f83-4ea6-4903-908e-592f4ad33789",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(path):\n",
    "    os.makedirs(path)\n",
    "    \n",
    "for file in ['true_rank','actioned_rank','provided_metrics','approx_metrics','state','cost_fn']:\n",
    "    fname = os.path.join(path,file + '.pkl')\n",
    "    records[file].to_pickle(fname)\n",
    "    print(f'Saved records for {file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e487e75c-8d20-4490-aa9f-23900f4dc16d",
   "metadata": {},
   "source": [
    "## Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e3300-eae8-4d9e-bb8d-0fd78b57fa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = {'balanced':{},\n",
    "          'mild_unbalanced':{},\n",
    "          'strong_unbalanced':{}}\n",
    "mu_dict = {0:np.array([1.,1.]),\n",
    "           1:np.array([0., 1.]),\n",
    "           2:np.array([1., 0.]),\n",
    "          3:np.array([0.5, 0.2]),\n",
    "          4:np.array([0.4,0.2]), \n",
    "          5:np.array([0.3,0.2]),\n",
    "          6:np.array([0.1,0.1])} \n",
    "\n",
    "for dataset in records.keys():\n",
    "    print(f'Dataset: {dataset}')\n",
    "    path = os.path.join(cd.PROJECT_DIR,'records', f'{dataset}/')\n",
    "    records_list = ['true_rank',  # true rank (unmodified output of the matchmaking alg)\n",
    "                    'actioned_rank', # rank resulting from the application of the the actions found in the previous search to the true rank\n",
    "                    'provided_metrics',  # metrics of the actioned rank (which is the one provided to the user)\n",
    "                    'approx_metrics', # metrics of the approximated rank (it's obtained by applying the actions found in the current search to the actioned rank)\n",
    "                    'state', # record of the dynamical system evolution for each query (it's equal to the metrics itself if no search was performed)\n",
    "                    'cost_fn']  # approximation error (it's equal to -1 if no search was performed)\n",
    "\n",
    "    for file in records_list:\n",
    "        fname = os.path.join(path,file + '.pkl')\n",
    "        # Load existing records (if any)\n",
    "        if os.path.isfile(fname):\n",
    "            records[dataset][file] = pd.read_pickle(fname)\n",
    "            print(f'Records loaded for {file}, with {len(records[dataset][file].columns)} experiments')\n",
    "        else:\n",
    "            print(f'No records found for {file} in {path}')\n",
    "            records[dataset][file] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f709de-3fa8-4215-b093-2f8bda850035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 19\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "imgpath = os.path.join(cd.PROJECT_DIR,'images', f'comparison/')\n",
    "\n",
    "if not os.path.isdir(imgpath):\n",
    "    os.makedirs(imgpath)\n",
    "    \n",
    "metrics_names= ['DIDI',\n",
    "               'rank_dist']\n",
    "\n",
    "colors = {'balanced':'green',\n",
    "          'mild_unbalanced':'#b100e9',\n",
    "          'strong_unbalanced':'red'}\n",
    "\n",
    "lighter_colors = {'balanced':'#52be80',\n",
    "          'mild_unbalanced':'#cb75e7',\n",
    "          'strong_unbalanced':'#ff6f6f'}\n",
    "\n",
    "markers = {'balanced':'.',\n",
    "          'mild_unbalanced':'x',\n",
    "          'strong_unbalanced':'^'}\n",
    "\n",
    "for mu in mu_dict:\n",
    "    print('*'*40)\n",
    "    print('EXP.',mu)\n",
    "    print('*'*40)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18,6), dpi=130)\n",
    "    fname = f'exp_{mu}.eps'\n",
    "    # Loop on metrics\n",
    "    for k, name in enumerate(metrics_names):\n",
    "        if name=='rank_dist':\n",
    "            name_ = 'Rank Distance'\n",
    "        else:\n",
    "            name_=name\n",
    "        # Loop on dataset\n",
    "        for dataset in ['balanced','mild_unbalanced','strong_unbalanced']:\n",
    "            if dataset=='mild_unbalanced':\n",
    "                dt = 'Mild Unbalanced'\n",
    "            elif dataset=='strong_unbalanced':\n",
    "                dt = 'Strong Unbalanced'\n",
    "            elif dataset=='balanced':\n",
    "                dt = 'Balanced'\n",
    "            rec = records[dataset]\n",
    "            n_sample = len(rec['provided_metrics'][mu]['actionable'][name])\n",
    "            ma_actionable =  pd.Series(rec['provided_metrics'][mu]['actionable'][name])\n",
    "            print(dataset, name, np.mean(rec['provided_metrics'][mu]['actionable'][name]))\n",
    "            axes[k].plot(range(n_sample), \n",
    "                     ma_actionable,\n",
    "                     marker=markers[dataset],\n",
    "                     linestyle='dotted',\n",
    "                     color=lighter_colors[dataset], \n",
    "                     label=f'{dt}')\n",
    "            \n",
    "            if name == 'rank_dist':\n",
    "                axes[k].set_ylim(-0.05,.4)\n",
    "            else:\n",
    "                axes[k].set_ylim(-0.05,1)\n",
    "\n",
    "\n",
    "            axes[k].axhline(y=np.mean(rec['provided_metrics'][mu]['actionable'][name]),\n",
    "                        color=colors[dataset], \n",
    "                        linestyle='-.',\n",
    "                        linewidth=2.5,\n",
    "                        label=f'{dt} (mean)')\n",
    "        if mu != 0:\n",
    "            axes[k].axhline(y=mu_dict[mu][k], color='blue', linewidth=2.5, linestyle='-', label='Threshold')\n",
    "        else:\n",
    "            axes[k].axhline(y=2., color='blue', linewidth=2.5, linestyle='-', label='Threshold')\n",
    "        axes[k].set_xlabel('Queries')\n",
    "        axes[k].set_ylabel(f'{name_}')\n",
    "        if name == 'rank_dist' and mu==0:\n",
    "            legend = axes[k].legend(loc='upper right',frameon=1, shadow=True)\n",
    "            legend.get_frame().set_facecolor('white')\n",
    "        elif name == 'DIDI' and mu==3:\n",
    "            legend = axes[k].legend(bbox_to_anchor=(1,1.3),frameon=1, shadow=True)\n",
    "            legend.get_frame().set_facecolor('white')\n",
    "    plt.savefig(os.path.join(imgpath, fname), format='eps', bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c39e962-b77a-42bd-9478-8ce70fc7d665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caus",
   "language": "python",
   "name": "caus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
